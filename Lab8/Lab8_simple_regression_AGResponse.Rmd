---
title: "Introduction to linear regression"
author: "Alyssa Gurkas"
output:
  pdf_document: default
  html_document:
    includes:
      in_header: header.html
    css: ./lab.css
    highlight: pygments
    theme: cerulean
    toc: true
    toc_float: true
editor_options: 
  chunk_output_type: console
---

```{r global_options, include=FALSE}
knitr::opts_chunk$set(eval = TRUE, message = FALSE, warning = FALSE)
```

The Human Freedom Index is a report that attempts to summarize the idea of 
"freedom" through a bunch of different variables for many countries around 
the globe. It serves as a rough objective measure for the relationships 
between the different types of freedom - whether it's political, religious, 
economical or personal freedom - and other social and economic circumstances. 
The Human Freedom Index is an annually co-published report by the 
Cato Institute, the Fraser Institute, and the Liberales Institut at 
the Friedrich Naumann Foundation for Freedom.

In this lab, you'll be analyzing data from Human Freedom Index reports 
from 2008-2016. Your aim will be to summarize a few of the relationships 
within the data both graphically and numerically in order to find which 
variables can help tell a story about freedom.

## Getting Started

### Load packages

In this lab, you will explore and visualize the data using the **tidyverse** 
suite of packages. The data can be found in the companion package for 
OpenIntro resources, **openintro**.

Let's load the packages.

```{r load-packages, message=FALSE}
library(tidyverse)
library(openintro)
library(DATA606)
library(car)
data('hfi', package='openintro')
```


### The data

The data we're working with is in the openintro package and it's called `hfi`, 
short for Human Freedom Index.

1.  What are the dimensions of the dataset?
  
**Exercise 1 Response**
  
**The dataset 'hfi' has 1458 observations. Each observation represents the** 
**annual data for the respective nation included in the Human Freedom Index** 
**reports from 2008-2016. There are 123 different variables in this dataset** 
**that describe the idea of "freedom", such as political, religious, economic,** 
**or personal freedom.**
  
**The dataset has 1458 rows and 123 columns.**

2.  What type of plot would you use to display the relationship between the 
    personal freedom score, `pf_score`, and one of the other numerical 
    variables? Plot this relationship using the variable `pf_expression_control`
    as the predictor. Does the relationship look linear? If you knew a country's
    `pf_expression_control`,or its score out of 10, with 0 being the most, 
    of political pressures and controls on media content, would you be 
    comfortable using a linear model to predict the personal freedom score?
  
**Exercise 2 Response**
  
```{r exercise-2-response, echo=TRUE}
ggplot(hfi, aes(x=pf_expression_control , y=pf_score)) +
  geom_point()
```
  
**To determine if there is a relationship between the personal freedom score** 
**and the personal freedom expression control score a scatter plot can be used** 
**to visualize the linear relationship. Based on the plot produced, there is**
**evidence that there is a relationship. Therefore, I would be comfortable**
**using a linear model to predict the personal freedom score.**

If the relationship looks linear, we can quantify the strength of the 
relationship with the correlation coefficient.

```{r cor}
hfi %>%
  summarise(cor(pf_expression_control, pf_score, use = "complete.obs"))
```

Here, we set the `use` argument to "complete.obs" since there are some 
observations of NA.

## Sum of squared residuals

<div id="boxedtext">
In this section, you will use an interactive function to investigate what we 
mean by "sum of squared residuals". You will need to run this function in your 
console, not in your markdown document. Running the function also requires that 
the `hfi` dataset is loaded in your environment.
</div>

Think back to the way that we described the distribution of a single variable. 
Recall that we discussed characteristics such as center, spread, and shape. 
It's also useful to be able to describe the relationship of two numerical 
variables, such as `pf_expression_control` and `pf_score` above.

3.  Looking at your plot from the previous exercise, describe the relationship 
    between these two variables. Make sure to discuss the form, direction, and 
    strength of the relationship as well as any unusual observations.
  
**Exercise 3 Response**
  
**There is a somewhat strong, positive linear relationship between the personal** 
**freedom expression control and personal freedom score.**

Just as you've used the mean and standard deviation to summarize a single 
variable, you can summarize the relationship between these two variables by 
finding the line that best follows their association. 
Use the following interactive function to select the line that you think does 
the best job of going through the cloud of points.

```{r plotss-expression-score, eval=FALSE}
# This will only work interactively (i.e. will not show in the knitted document)
hfi <- hfi %>% filter(complete.cases(pf_expression_control, pf_score))
DATA606::plot_ss(x = hfi$pf_expression_control, y = hfi$pf_score)
```

After running this command, you'll be prompted to click two points on the plot 
to define a line. Once you've done that, the line you specified will be shown 
in black and the residuals in blue. Note that there are 30 residuals, 
one for each of the 30 observations. Recall that the residuals are the 
difference between the observed values and the values predicted by the line:

\[
  e_i = y_i - \hat{y}_i
\]

The most common way to do linear regression is to select the line that minimizes
the sum of squared residuals. To visualize the squared residuals, you can rerun 
the plot command and add the argument `showSquares = TRUE`.

```{r plotss-expression-score-squares, eval=FALSE}
DATA606::plot_ss(x = hfi$pf_expression_control, y = hfi$pf_score, showSquares = TRUE)
```

Note that the output from the `plot_ss` function provides you with the slope 
and intercept of your line as well as the sum of squares.

4.  Using `plot_ss`, choose a line that does a good job of minimizing the sum of
    squares. Run the function several times. What was the smallest sum of 
    squares that you got? How does it compare to your neighbors?
  
**Exercise 4 Response**

```{r exercise-4-response, echo=TRUE}
  # plot_ss(hfi$pf_expression_control, 
  #         hfi$pf_score, 
  #         showSquares=FALSE, 
  #         leastSquares=FALSE)
```

**The lowest sum of squares produced in this exercise was 1068.176. Other people**
**in the class had lower sums of squares, such as 953.**

## The linear model

It is rather cumbersome to try to get the correct least squares line, i.e. the 
line that minimizes the sum of squared residuals, through trial and error. 
Instead, you can use the `lm` function in R to fit the linear model 
(a.k.a. regression line).

```{r m1}
m1 <- lm(pf_score ~ pf_expression_control, data = hfi)
```

The first argument in the function `lm` is a formula that takes the form 
`y ~ x`. Here it can be read that we want to make a linear model of `pf_score` 
as a function of `pf_expression_control`. The second argument specifies that R 
should look in the `hfi` data frame to find the two variables.

The output of `lm` is an object that contains all of the information we need 
about the linear model that was just fit. We can access this information using 
the summary function.

```{r summary-m1}
summary(m1)
```

Let's consider this output piece by piece. First, the formula used to describe
the model is shown at the top. After the formula you find the five-number 
summary of the residuals. The "Coefficients" table shown next is key; its first
column displays the linear model's y-intercept and the coefficient of 
`pf_expression_control`. With this table, we can write down the least squares 
regression line for the linear model:

\[
  \hat{y} = 4.61707 + 0.49143 \times pf\_expression\_control
\]

One last piece of information we will discuss from the summary output is the 
Multiple R-squared, or more simply, $R^2$. The $R^2$ value represents the 
proportion of variability in the response variable that is explained by the 
explanatory variable. For this model, 63.42% of the variability in runs is 
explained by at-bats.

5.  Fit a new model that uses `pf_expression_control` to predict `hf_score`, or
    the total human freedom score. Using the estimates from the R output, write 
    the equation of the regression line. What does the slope tell us in the 
    context of the relationship between human freedom and the amount of
    political pressure on media content?
  
**Exercise 5 Response**  

```{r exercise-5-response, echo=TRUE}
ggplot(hfi, aes(x=pf_expression_control, y=hf_score)) +
  geom_point()

hfi %>%
  summarise(cor(pf_expression_control, hf_score, use = "complete.obs"))


```

**The slope provided in the R output from the lm() function is 0.349862.** 
**Using the values provided in the output from 'lm' we can produce the** 
**following equation of the regression line:**

$y = 5.153687 + 0.349862 * pf expression control$

  

**The slope tells us that for each additional point in the political expression**
**indicator we can expect roughly 0.349862 additional points in the human freedom**
**score. This means that if a nation were to have a higher personal freedom**
**expression score, you can expect that it has a relatively high human**
**freedom score.**

## Prediction and prediction errors

Let's create a scatterplot with the least squares line for `m1` laid on top.

```{r reg-with-line}
ggplot(data = hfi, aes(x = pf_expression_control, y = pf_score)) +
  geom_point() +
  stat_smooth(method = "lm", se = FALSE)
```

Here, we are literally adding a layer on top of our plot. 
`geom_smooth` creates the line by fitting a linear model. 
It can also show us the standard error `se` associated with our line, 
but we'll suppress that for now.

This line can be used to predict $y$ at any value of $x$. When predictions are 
made for values of $x$ that are beyond the range of the observed data, it is 
referred to as *extrapolation* and is not usually recommended. 
However, predictions made within the range of the data are more reliable. 
They're also used to compute the residuals.

6.  If someone saw the least squares regression line and not the actual data, 
how would they predict a country's personal freedom school for one with a 
6.7 rating for `pf_expression_control`? Is this an overestimate or an 
underestimate, and by how much? 
In other words, what is the residual for this prediction?

  

**Exercise 6 Response**
```{r exercise-6-response, echo=TRUE}

result <- 5.153687 + 0.349862 * 6.7
result

ggplot(data = hfi, aes(x = pf_expression_control, y = hf_score)) +
  geom_point() +
  stat_smooth(method = "lm", se = FALSE)
```
  
**If someone saw the least squares regression line and not the actual data,**
**they would predict a human freedom score of ~7.5.**
  
**Based on the graph shown above, the model is an overestimate.** 
**We can determine how much the model is overestimating by reviewing the** 
**output of the m2 summary. In the summary table, it shows that there is a**
**residual standard error of 0.667 on 1376 degrees of freedom and notes that**
**80 observations were removed due to missing data. We can conclude that this**
**model overestimates by 0.667 for the human freedom score, on average.**


## Model diagnostics

To assess whether the linear model is reliable, we need to check for 
(1) linearity, (2) nearly normal residuals, and (3) constant variability.

**Linearity**: You already checked if the relationship between `pf_score` 
and `pf_expression_control' is linear using a scatterplot. 
We should also verify this condition with a plot of the residuals vs. 
fitted (predicted) values.

```{r residuals}
ggplot(data = m1, aes(x = .fitted, y = .resid)) +
  geom_point() +
  geom_hline(yintercept = 0, linetype = "dashed") +
  xlab("Fitted values") +
  ylab("Residuals")
```

Notice here that `m1` can also serve as a data set because stored within it are 
the fitted values ($\hat{y}$) and the residuals. Also note that we're getting 
fancy with the code here. After creating the scatterplot on the first 
layer (first line of code), we overlay a horizontal dashed line at $y = 0$ 
(to help us check whether residuals are distributed around 0), 
and we also rename the axis labels to be more informative.

7.  Is there any apparent pattern in the residuals plot? What does this indicate
    about the linearity of the relationship between the two variables?  
  
**Exercise 7 Response**
  
**There is one pattern detected in the residuals plot. At the upper end of the** 
**fitted values distribution, the residuals are almost entirely positive.**
**The trend appears to be linear and the data falls around the line with no**
**obvious outliers. If we were to notice a curvature, we should not use a**
**straight line to model the data. In the event that we wanted to complete**
**additional assessments to ensure the relationship is linear we can use the** 
**residualPlot() function from the car package, adding a curvature test:**
  
```{r exercise-7-a-response, echo=TRUE}
car::residualPlot(model=m2)
```

```{r exercise-7-b-response, echo=TRUE}
car::residualPlots(model=m2)
```
  
**From these plots, we can see that there is a slight curvature although it** 
**seems that the deviations from linearity are decently small, and may not be** 
**worth any concern. The third line on these plots are the**
**Tukey tests which square the fitted-value.**

**Nearly normal residuals**: To check this condition, we can look at a histogram

```{r hist-res}
ggplot(data = m1, aes(x = .resid)) +
  geom_histogram(bins = 25) +
  xlab("Residuals") 
```

or a normal probability plot of the residuals.

```{r qq-res}
ggplot(data = m1, aes(sample = .resid)) +
  stat_qq()
```

Note that the syntax for making a normal probability plot is a bit different 
than what you're used to seeing: we set `sample` equal to the residuals instead 
of `x`, and we set a statistical method `qq`, which stands for 
"quantile-quantile", another name commonly used for normal probability plots.

8.  Based on the histogram and the normal probability plot, does the nearly 
    normal residuals condition appear to be met?
  
**Exercise 8 Response**
  
**Based on the histogram and the normal probability plot, the residuals are** 
**nearly normal but are slightly left skewed. There are not any residuals that**
**are much further from the regression line than others. However, it should be** 
**noted that there are a few residuals that may be considered outliers, although** 
**the spread is still evenly distributed, and the data seems nearly normal**
**enough.**
  

**Constant variability**:

9.  Based on the residuals vs. fitted plot, does the constant variability 
    condition appear to be met?
  
**Exercise 9 Response**
  
**Based on the residuals vs. fitted plot, the constant variability condition**
**does appear to be met. If we wanted to conduct further analysis to confirm,**
**we could complete a non-constant variance test using the car package:**

```{r exercise-9-response, echo=TRUE}
car::ncvTest(m2)
```
  

**When conducting a Non-Constant Variance Test using the ncvTest() function**
**from the car package, it seems that there is not homoscedasticity. Based on**
**this finding, I would be inclined to use a "heteroscedasticity corrected**
**covariance matrix" when estimating standard errors.**

* * *

## More Practice

-   Choose another freedom variable and a variable you think would strongly
    correlate with it.. Produce a scatterplot of the two variables and fit a 
    linear model. At a glance, does there seem to be a linear relationship?

```{r exercise-10-response, echo=TRUE}
ggplot(hfi, aes(x=ef_regulation_labor, y=hf_score)) +
  geom_point()
```

**At a glance, it seems that there may be a positive, somewhat linear**
**relationship, but it does not seem very strong. We can further investigate** 
**by calculating the correlation:**

```{r exercise-10-a-response, echo=TRUE}
hfi %>%
  summarise(cor(ef_regulation_labor, hf_score, use = "complete.obs"))
```
  
**Based on this result, we can confirm the correlation is positive, but is**
**in fact weak, with R=0.325.**
  
```{r exercise-10-b-response, echo=TRUE}

m3 <- lm(hf_score ~ ef_regulation_labor, data = hfi)
summary(m3)

ggplot(data = hfi, aes(x = ef_regulation_labor, y = hf_score)) +
  geom_point() +
  stat_smooth(method = "lm", se = FALSE)
```

-   How does this relationship compare to the relationship between 
    `pf_expression_control` and `pf_score`? Use the $R^2$ values from the two 
    model summaries to compare. Does your independent variable seem to predict
    your dependent one better? Why or why not?
  
**Exercise 11 Response**
  
**The** $R^2$ **value for the third model is 0.1058. This means that a 10.58% of** 
**the variation in the human freedom score can be explained by the economic**
**freedom of regulation of labor score. When comparing this result to model 2,**
**where we compared the personal freedom score and the control of expression's** 
**(`pf_expression_control`) ability to predict the human freedom score**
**(`hf_score`), we may be able to conclude that model 2 is stronger.**
**Model 2's** $R^2$ **value is 0.5775.**

-   What's one freedom relationship you were most surprised about and why? Display
    the model diagnostics for the regression model analyzing this relationship.
  
**Exercise 12 Response**
  
**I was very surprised to see that there was not a relationship between the**
**score for press killed and the safety and security of women. In the code**
**below, I attempted to fit a model that uses `pf_expression_killed` to predict**
**the safety and security of women, however, there is not a relationship**
**between the two indicators. This model proves to be a very bad model that**
**likely cannot be used as a prediction tool.**
  
```{r exercise-12-response, echo=TRUE}
ggplot(hfi, aes(x=pf_expression_killed, y=pf_ss_women)) +
  geom_point()

m4 <- lm(pf_ss_women ~ pf_expression_killed, data = hfi)
summary(m4)

ggplot(data = hfi, aes(x = pf_expression_killed, y = pf_ss_women)) +
  geom_point() +
  stat_smooth(method = "lm", se = FALSE)
```

* * *
